import streamlit as st
import pandas as pd
import pickle
import numpy as np
import re
import os
import requests
from PIL import Image
from io import BytesIO
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image
from sklearn.metrics.pairwise import cosine_similarity

# --- [1] êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬ ë³€í™˜ ë° ë°ì´í„° ë¡œë“œ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼) ---
def get_direct_url(url):
    if not url or str(url) == 'nan' or 'drive.google.com' not in url:
        return url
    if 'file/d/' in url:
        file_id = url.split('file/d/')[1].split('/')[0]
    elif 'id=' in url:
        file_id = url.split('id=')[1].split('&')[0]
    else: return url
    return f'https://drive.google.com/uc?export=download&id={file_id}'

def load_csv_smart(target_name):
    files = os.listdir('.')
    for f in files:
        if f.lower() == target_name.lower():
            for enc in ['utf-8-sig', 'utf-8', 'cp949', 'euc-kr']:
                try: return pd.read_csv(f, encoding=enc)
                except: continue
    st.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_name}")
    st.stop()

@st.cache_resource
def init_resources():
    model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
    with open('material_features.pkl', 'rb') as f:
        feature_db = pickle.load(f)
    df_path = load_csv_smart('ì´ë¯¸ì§€ê²½ë¡œ.csv')
    df_info = load_csv_smart('í’ˆëª©ì •ë³´.csv')
    df_stock = load_csv_smart('í˜„ì¬ê³ .csv')
    df_stock['ì¬ê³ ìˆ˜ëŸ‰'] = pd.to_numeric(df_stock['ì¬ê³ ìˆ˜ëŸ‰'].astype(str).str.replace(',', ''), errors='coerce').fillna(0)
    df_stock['í’ˆë²ˆ_KEY'] = df_stock['í’ˆë²ˆ'].astype(str).str.strip().str.upper()
    agg_stock = df_stock.groupby('í’ˆë²ˆ_KEY')['ì¬ê³ ìˆ˜ëŸ‰'].sum().to_dict()
    stock_date = str(int(df_stock['ì •ì‚°ì¼ì'].max())) if 'ì •ì‚°ì¼ì' in df_stock.columns else "í™•ì¸ë¶ˆê°€"
    return model, feature_db, df_path, df_info, agg_stock, stock_date

model, feature_db, df_path, df_info, agg_stock, stock_date = init_resources()

# --- [2] ë§¤ì¹­ ë³´ì¡° í•¨ìˆ˜ ---
def get_digits(text):
    if not text or pd.isna(text): return ""
    return "".join(re.findall(r'\d+', str(text)))

@st.cache_data
def get_master_map():
    mapping = {}
    for _, row in df_info.iterrows():
        f_code, l_no, p_name = str(row['ìƒí’ˆì½”ë“œ']).strip(), str(row['Lab No']).strip(), str(row['ìƒí’ˆëª…']).strip()
        k_lab, k_formal = get_digits(l_no), get_digits(f_code)
        val = {'formal': f_code, 'name': p_name}
        if k_lab: mapping[k_lab] = val
        if k_formal: mapping[k_formal] = val
    return mapping

master_map = get_master_map()

# --- [3] UI êµ¬ì„± ---
st.set_page_config(layout="wide", page_title="ìì¬ íŒ¨í„´ ë§¤ì¹­")
st.title("ğŸ­ ìì¬ íŒ¨í„´ ê²€ìƒ‰ ë° ì‹¤ì‹œê°„ ì¬ê³  í™•ì¸")
st.sidebar.info(f"ğŸ“… ì¬ê³  ê¸°ì¤€ì¼: {stock_date}")

uploaded = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['jpg', 'jpeg', 'png', 'tif', 'tiff'])

if uploaded:
    # ğŸ–¼ï¸ [ì¶”ê°€ ê¸°ëŠ¥] íƒ€ê²Ÿ ì´ë¯¸ì§€ í¼ì¹˜ê³  ë‹«ê¸° (ê²°ê³¼ ë¶„ì„ ì¤‘ì—ë„ í™•ì¸ ê°€ëŠ¥)
    # ì²˜ìŒì—ëŠ” í¼ì³ì ¸ ìˆê²Œ(expanded=True) ì„¤ì •í–ˆìŠµë‹ˆë‹¤.
    with st.expander("ğŸ“¸ ë‚´ê°€ ì—…ë¡œë“œí•œ íƒ€ê²Ÿ ì´ë¯¸ì§€ í™•ì¸", expanded=True):
        col_target, col_empty = st.columns([1, 2])
        with col_target:
            st.image(uploaded, use_container_width=True, caption="ê²€ìƒ‰ì˜ ê¸°ì¤€ ì´ë¯¸ì§€")
        with col_empty:
            st.write("ìœ„ ì´ë¯¸ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬í•œ íŒ¨í„´ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            st.write("ê²°ê³¼ë¥¼ ë³´ì‹¤ ë•Œ ì´ ì°½ì„ ì ‘ìœ¼ë©´ í™”ë©´ì„ ë” ë„“ê²Œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    with st.spinner('ìœ ì‚¬ íŒ¨í„´ê³¼ ì‹¤ì¬ê³  ëŒ€ì¡° ì¤‘...'):
        target_img = Image.open(uploaded).convert('RGB').resize((224, 224))
        x = image.img_to_array(target_img)
        x = np.expand_dims(x, axis=0)
        query_vec = model.predict(preprocess_input(x), verbose=0).flatten().reshape(1, -1)
        
        db_names, db_vecs = list(feature_db.keys()), np.array(list(feature_db.values()))
        sims = cosine_similarity(query_vec, db_vecs).flatten()
        
        results = []
        for i in range(len(db_names)):
            fname = db_names[i]
            info = master_map.get(get_digits(fname), {'formal': fname, 'name': 'ì •ë³´ ì—†ìŒ'})
            formal_code = info['formal']
            qty = agg_stock.get(formal_code.strip().upper(), 0)
            url_row = df_path[df_path['íŒŒì¼ëª…'] == fname]
            url = url_row['ì¹´ì¹´ì˜¤í†¡_ì „ì†¡ìš©_URL'].values[0] if not url_row.empty else None
            results.append({'formal': formal_code, 'name': info['name'], 'score': sims[i], 'stock': qty, 'url': url})
        
        results = sorted(results, key=lambda x: x['score'], reverse=True)

    # --- [4] ê²°ê³¼ ì¶œë ¥ ---
    def display_card(item, idx):
        st.markdown(f"**{idx}. {item['formal']}**")
        st.write(f"í’ˆëª…: {item['name']}")
        st.write(f"ìœ ì‚¬ë„: {item['score']:.1%}")
        with st.expander("ğŸ–¼ï¸ ì´ë¯¸ì§€ ë³´ê¸°", expanded=False):
            if item['url']:
                try:
                    res = requests.get(get_direct_url(item['url']), timeout=10)
                    st.image(Image.open(BytesIO(res.content)), use_container_width=True)
                except: st.write("âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")
            else: st.write("ì´ë¯¸ì§€ ì—†ìŒ")
        
        if item['stock'] >= 100: st.success(f"ì¬ê³ : {item['stock']:,}m")
        else: st.write(f"ì¬ê³ : {item['stock']:,}m")

    tab1, tab2 = st.tabs(["ğŸ“Š ì „ì²´ ê²€ìƒ‰ ê²°ê³¼", "âœ… ì¬ê³  ìˆìŒ (100mâ†‘)"])
    with tab1:
        cols = st.columns(5)
        for i, r in enumerate(results[:10]):
            with cols[i % 5]: display_card(r, i + 1)
    with tab2:
        in_stock = [r for r in results if r['stock'] >= 100]
        if in_stock:
            cols = st.columns(5)
            for i, r in enumerate(in_stock[:10]):
                with cols[i % 5]: display_card(r, i + 1)
        else: st.warning("ì¬ê³ ê°€ ì¶©ë¶„í•œ ìì¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
